{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785604fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 11:35:13.370874: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-14 11:35:13.401415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-14 11:35:13.853757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ivan/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, GRU, Bidirectional, Concatenate, Dropout, Layer, Add, LayerNormalization\n",
    "from keras.utils import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk import edit_distance\n",
    "\n",
    "from funkcije import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdbd064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 11:35:14.960952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:14.976847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:14.976995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "#Mozda resi problem sa memorijom GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model_weights_{epoch}.h5', save_best_only=False, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "#Ckeckpoint se vise ne koristi\n",
    "early_stopping = EarlyStopping(patience = 10, restore_best_weights = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n",
    "\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3962314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding size, input_pad_len i target_pad_len su fiksni\n",
    "#Dropout se primenjuje na vise mesta, i svuda je isti dropout_rate\n",
    "#Na samom pocetku modelu se primenjuje custom_dropout, kako bi naucio da radi sa <Unknown> tokenom bolje\n",
    "#Encoder ima 3 GRU sloja, poslendji je dvosmeran; izmedju svaka 2 postoje rezidualne veze\n",
    "#Posto je poslednji sloj decodera dvosmeran GRU, latentna dimenzija dekodera je duplo veca\n",
    "#Deocoder ima 2 GRU sloja, izmedju postoje rezidualne veze\n",
    "#Metoda translate sekvencijalni prevodi podatke rec po rec, zbog toga poziva dekoder onoliko puta, koliko je maksimalna duzina target recenice\n",
    "class GRU_Translation_Model(Model):\n",
    "    def __init__(self, num_input_words, num_target_words, input_embedding_matrix, target_embedding_matrix, latent_dim = 256, dropout_rate = 0.5, custom_dropout_rate = 0.05):\n",
    "        super(GRU_Translation_Model, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.custom_dropout_rate = custom_dropout_rate\n",
    "        self.num_input_words = num_input_words\n",
    "        self.num_target_words = num_target_words\n",
    "        self.input_embedding_matrix = input_embedding_matrix\n",
    "        self.target_embedding_matrix = target_embedding_matrix\n",
    "        self.embedding_size = 300\n",
    "        self.input_pad_len = 80\n",
    "        self.target_pad_len = 60\n",
    "        \n",
    "        encoder_input_tensor = Input(shape = (self.input_pad_len, ))\n",
    "        modified_input = CustomDropout(1.0, custom_dropout_rate)(encoder_input_tensor)\n",
    "\n",
    "        encoder_embedding_layer = Embedding(input_dim = num_input_words + 1, output_dim = self.embedding_size, mask_zero = True, weights = [input_embedding_matrix], trainable = True)\n",
    "        encoder_embedding = encoder_embedding_layer(modified_input)\n",
    "        outputs = GRU(units = latent_dim, return_sequences = True, dropout = dropout_rate)(encoder_embedding)\n",
    "        outputs = Dense(units = self.embedding_size, activation = 'relu')(outputs)\n",
    "        outputs = LayerNormalization()(outputs)\n",
    "        #outputs = Dropout(dropout_rate)(outputs)\n",
    "        next_inputs = Add()([encoder_embedding, outputs])\n",
    "        outputs = GRU(units = latent_dim, return_sequences = True, dropout = dropout_rate)(next_inputs)\n",
    "        outputs = Dense(units = self.embedding_size, activation = 'relu')(outputs)\n",
    "        outputs = LayerNormalization()(outputs)\n",
    "        main_inputs = Add()([next_inputs, outputs])\n",
    "        main_inputs = LayerNormalization()(main_inputs)\n",
    "        _, forward_state, backward_state = Bidirectional(GRU(units = latent_dim, return_state = True, dropout = dropout_rate))(main_inputs)\n",
    "        state_h = Concatenate(axis=-1)([forward_state, backward_state])\n",
    "\n",
    "        self.encoder = Model(encoder_input_tensor, state_h)\n",
    "        \n",
    "        decoder_input_tensor = Input(shape = (None, ))\n",
    "        decoder_starting_state = Input(shape = (latent_dim*2,))\n",
    "        decoder_embedding_layer = Embedding(input_dim = num_target_words + 1, output_dim = self.embedding_size, mask_zero = True, weights = [target_embedding_matrix], trainable = True)\n",
    "        decoder_embedding = decoder_embedding_layer(decoder_input_tensor)\n",
    "        decoder_outputs = GRU(units = latent_dim*2, return_sequences = True, return_state = False, dropout = dropout_rate)(decoder_embedding, initial_state = decoder_starting_state)\n",
    "        decoder_outputs = Dense(units = self.embedding_size, activation = 'relu')(decoder_outputs)\n",
    "        main_inputs = Add()([decoder_embedding, decoder_outputs])\n",
    "        main_inputs = LayerNormalization()(main_inputs)\n",
    "        decoder_outputs, decoder_state = GRU(units = latent_dim*2, return_sequences = True, return_state = True, dropout = dropout_rate)(main_inputs, initial_state = decoder_starting_state) \n",
    "        output = Dropout(dropout_rate)(decoder_outputs)\n",
    "        output = Dense(units = num_target_words + 1, activation = 'softmax')(output)\n",
    "        \n",
    "        self.decoder = Model([decoder_input_tensor, decoder_starting_state], [output, decoder_state])\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoder_output = self.encoder(x[0])\n",
    "        decoder_output,_ = self.decoder([x[1],encoder_output])\n",
    "        return decoder_output\n",
    "    \n",
    "    #Encodes encoder_input and then decodes sequentially, with decoder_input as the starting input\n",
    "    def translate(self, encoder_input, decoder_input):\n",
    "        decoder_state = self.encoder(encoder_input)\n",
    "        data_size = encoder_input.shape[0]\n",
    "        decoder_output = np.zeros((data_size, self.target_pad_len - 1))\n",
    "        for i in range(self.target_pad_len - 1):\n",
    "            decoder_output_temp, decoder_state = self.decoder.predict([decoder_input, decoder_state], verbose = 0, batch_size = 128)\n",
    "            next_words = np.argmax(decoder_output_temp, axis = -1)\n",
    "            decoder_input = next_words\n",
    "            decoder_output[:, i] = next_words.reshape((data_size,))\n",
    "        return decoder_output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdecb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trenira model na train_data i evaluira ga na val_data\n",
    "#Embedding learning rate je poseban learning_rate koji se koristi u embedding slojevima, iz razloga sto oni vec imaju pretrenirane podatke za pocetne vrednosti\n",
    "#Model se trenira dok val_loss ne krene da raste, i cuva tezine epohe koja ima najbolji val_loss\n",
    "#Koristi se u cv_evaluate, napravio funkciju jer inace dolazi do prekoracenje GPU RAMa, neko je napisao da je do unakrsne validacije\n",
    "def train_and_evaluate(train_data, val_data, epochs = 200, batch_size = 128, learning_rate = 0.001, latent_dim = 256, dropout_rate = 0.5, embedding_learning_rate = 0.001):\n",
    "     \n",
    "     input_texts, target_texts = clean_texts(train_data.iloc[:,1], train_data.iloc[:,0])\n",
    "     input_word_index, target_word_index, max_input_seq_len, max_target_seq_len = analyse_texts(input_texts, target_texts)\n",
    "     input_pad_len = 80\n",
    "     target_pad_len = 60\n",
    "     num_input_words = len(input_word_index) - 1\n",
    "     num_target_words = len(target_word_index) - 1\n",
    "     #print(num_input_words)\n",
    "     inverted_input_word_index = {value: key for key,value in input_word_index.items()}\n",
    "     inverted_target_word_index = {value: key for (key,value) in target_word_index.items()}\n",
    "     #print(len(inverted_input_word_index))\n",
    "     input_embedding_matrix, target_embedding_matrix = load_embedding_data_get_matrices(inverted_input_word_index, inverted_target_word_index)\n",
    "     print('Embeddings loaded.')\n",
    "     encoder_input_data, decoder_input_data, decoder_output_data = create_model_data(input_texts, target_texts, input_word_index, target_word_index, input_pad_len, target_pad_len)\n",
    "     #print(input_embedding_matrix.shape)\n",
    "     \n",
    "     input_texts_val, target_texts_val = clean_texts(val_data.iloc[:,1], val_data.iloc[:,0])\n",
    "     encoder_input_data_val, decoder_input_data_val, decoder_output_data_val = create_model_data(input_texts_val, target_texts_val, input_word_index, target_word_index, input_pad_len, target_pad_len)\n",
    "     \n",
    "     #print('Data preprocessed.')\n",
    "     model_gru = GRU_Translation_Model(num_input_words, num_target_words, input_embedding_matrix, target_embedding_matrix, latent_dim = latent_dim, dropout_rate = dropout_rate)\n",
    "     #print('Model loaded.')\n",
    "     other_layers = model_gru.layers[0].layers + model_gru.layers[1].layers #Mora da se prilagodi za transformer\n",
    "     embedding_layers = [other_layers.pop(2), other_layers.pop(-9)] #Paznja! Mora se prilagoditi svaki put kad se model menja\n",
    "\n",
    "     optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers = [(Adam(learning_rate), other_layers), (Adam(embedding_learning_rate), embedding_layers)])\n",
    "     model_gru.compile(optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "     #print('Model compiled.')\n",
    "     history = model_gru.fit([encoder_input_data, decoder_input_data], decoder_output_data, validation_data = ([encoder_input_data_val, decoder_input_data_val], decoder_output_data_val), epochs = epochs, batch_size = batch_size, callbacks = [early_stopping], verbose = 1)\n",
    "     #print('Model fit.')\n",
    "     best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "     \n",
    "     #print('Best epoch: ', best_epoch)\n",
    "     best_loss = np.min(history.history['val_loss'])\n",
    "     #print('Best loss:', best_loss)\n",
    "     #print(model_gru.evaluate([encoder_input_data_val, decoder_input_data_val], decoder_output_data_val))\n",
    "     \n",
    "     wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1 = evaluate(model_gru, input_texts_val, target_texts_val, input_word_index, target_word_index, inverted_target_word_index, input_pad_len, target_pad_len)\n",
    "     return best_epoch, best_loss, wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49dc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trenira po model za svaki fold, racuna WER, smooth BLEU(1,2,3,4), kao i val_loss i broj epoha do konvergencije\n",
    "#Vraca podatke iz svake instance modela, odnosno za svaki fold, da bi se dalje procesirale\n",
    "def cv_evaluate(train_val_data = None, df_folds = None, folds = 5, epochs = 200, batch_size = 128, learning_rate = 0.001, latent_dim = 256, dropout_rate = 0.5, embedding_learning_rate = None):\n",
    "    if embedding_learning_rate == None:\n",
    "        embedding_learning_rate = learning_rate\n",
    "    if df_folds == None:\n",
    "        df_np = train_val_data.to_numpy()\n",
    "        np.random.shuffle(df_np)\n",
    "        total_size = df_np.shape[0]\n",
    "        fold_size = total_size/folds\n",
    "        df_folds = [df_np[int(i*fold_size):int((i+1)*fold_size),] for i in range(folds)]\n",
    "    #input_word_embeddings, target_word_embeddings = load_embedding_data() #Doslo je do prekoracenje memorije\n",
    "    losses = []\n",
    "    best_epochs = []\n",
    "    wers = []\n",
    "    smooth_bleu1s = []\n",
    "    smooth_bleu2s = []\n",
    "    smooth_bleu3s = []\n",
    "    smooth_bleu4s = []\n",
    "    for i in range(folds):\n",
    "        train_folds = [fold for j, fold in enumerate(df_folds) if j!=i]\n",
    "        train_folds_pd = [pd.DataFrame(data = fold) for fold in train_folds]\n",
    "        train_data = pd.concat(train_folds_pd)\n",
    "        val_data = pd.DataFrame(df_folds[i])\n",
    "        print('Current Latent Dim:', latent_dim)\n",
    "        print('Current Dropout Rate: ', dropout_rate)\n",
    "        print('Current Fold: {}/{}'.format(i+1, folds))\n",
    "        print('Current Learning Rate: ', learning_rate)\n",
    "        print('Current Learning Rate Multiplier: ', embedding_learning_rate/learning_rate)\n",
    "        \n",
    "        best_epoch, best_loss, wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1 = train_and_evaluate(train_data, val_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, latent_dim = latent_dim, dropout_rate = dropout_rate, embedding_learning_rate = embedding_learning_rate)\n",
    "        best_epochs.append(best_epoch)\n",
    "        losses.append(best_loss)\n",
    "        wers.append(wer)\n",
    "        smooth_bleu4s.append(smooth_bleu4)\n",
    "        smooth_bleu3s.append(smooth_bleu3)\n",
    "        smooth_bleu2s.append(smooth_bleu2)\n",
    "        smooth_bleu1s.append(smooth_bleu1)\n",
    "    return best_epochs, losses, wers, smooth_bleu4s, smooth_bleu3s, smooth_bleu2s, smooth_bleu1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87ac883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluira modele za razlicite vrednosti latent_dim i dropout_rate\n",
    "#U 3d matrici cuva rezultate, treca dimenzija predstavlja vrednosti za razlicite foldove, uprosecavanjem se dobija zeljena metrika\n",
    "#Isto cuva i broj epoha do konvergencije \n",
    "def cv_grid_search(df, dropout_rates, latent_dims, epochs = 200, learning_rate = 0.0002, folds = 5):\n",
    "    df_np = df.to_numpy()\n",
    "    np.random.shuffle(df_np)\n",
    "    total_size = df_np.shape[0]\n",
    "    fold_size = total_size/folds\n",
    "    df_folds = [df_np[int(i*fold_size):int((i+1)*fold_size),] for i in range(folds)]\n",
    "    \n",
    "    loss_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    epoch_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    wer_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu4_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu3_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu2_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu1_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    for i in range(len(latent_dims)):\n",
    "        for j in range(len(dropout_rates)):\n",
    "            best_epochs, losses, wers, smooth_bleu4s, smooth_bleu3s, smooth_bleu2s, smooth_bleu1s = cv_evaluate(df_folds = df_folds, folds = folds, epochs = epochs, learning_rate = learning_rate, latent_dim = latent_dims[i], dropout_rate = dropout_rates[j])\n",
    "            print(losses)\n",
    "            print(best_epochs)\n",
    "            loss_matrix[i,j,:] = losses\n",
    "            epoch_matrix[i,j,:] = best_epochs\n",
    "            wer_matrix[i,j,:] = wers\n",
    "            smooth_bleu4_matrix[i,j,:] = smooth_bleu4s\n",
    "            smooth_bleu3_matrix[i,j,:] = smooth_bleu3s\n",
    "            smooth_bleu2_matrix[i,j,:] = smooth_bleu2s\n",
    "            smooth_bleu1_matrix[i,j,:] = smooth_bleu1s\n",
    "    #Pakuju se rezultati u dictionary radi intuitivnijeg poziva funkcije\n",
    "    metrics_dict = {'loss': loss_matrix, 'epoch': epoch_matrix, 'wer': wer_matrix, 'smooth_bleu4': smooth_bleu4_matrix, 'smooth_bleu3': smooth_bleu3_matrix, 'smooth_bleu2': smooth_bleu2_matrix, 'smooth_bleu1': smooth_bleu1_matrix }\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8105be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/PHOENIX-2014-T.train.corpus.csv', sep='|')\n",
    "df_train = df_train.drop(columns=['name','video','start','end','speaker'])\n",
    "train_size = df_train.shape[0]\n",
    "#Orth je glossovana recenica, translation je originalna engleska\n",
    "\n",
    "df_val = pd.read_csv('data/PHOENIX-2014-T.dev.corpus.csv', sep = '|')\n",
    "df_val.drop(columns = ['name', 'video', 'start', 'end', 'speaker'], inplace = True)\n",
    "val_size = df_val.shape[0]\n",
    "\n",
    "df_test = pd.read_csv('data/PHOENIX-2014-T.test.corpus.csv', sep = '|')\n",
    "df_test.drop(columns = ['name', 'video', 'start', 'end', 'speaker'], inplace = True)\n",
    "test_size = df_test.shape[0]\n",
    "\n",
    "df_train_val = pd.concat([df_train, df_val])\n",
    "df_full = pd.concat([df_train_val, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3396ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametri za optimizaciju: dropout rate i latentna dimenzija\n",
    "dropout_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "latent_dims = [256, 512, 1024] #Treba probati i vecu latentnu dimenziju i dropout rate, posto optimalna vrednost ispada najveca\n",
    "learning_rate = 0.0002\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc7cdfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.5\n",
      "Current Fold: 1/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 11:35:53.214843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:53.215025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:53.215105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:53.271354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:53.271482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:53.271561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-14 11:35:53.271622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9591 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 11:36:07.432629: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_41/output/_22'\n",
      "2023-09-14 11:36:07.608365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-09-14 11:36:07.658976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-14 11:36:08.468266: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff2b3142860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-14 11:36:08.468281: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2023-09-14 11:36:08.472218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-14 11:36:08.570036: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 29s 294ms/step - loss: 5.2482 - acc: 0.1522 - val_loss: 4.3672 - val_acc: 0.2217\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 5s 112ms/step - loss: 4.3524 - acc: 0.2307 - val_loss: 4.0141 - val_acc: 0.2704\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 3s 58ms/step - loss: 4.0637 - acc: 0.2670 - val_loss: 3.7459 - val_acc: 0.3009\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 2s 46ms/step - loss: 3.8395 - acc: 0.2936 - val_loss: 3.5613 - val_acc: 0.3193\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 3.6642 - acc: 0.3092 - val_loss: 3.4227 - val_acc: 0.3357\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 2s 40ms/step - loss: 3.5358 - acc: 0.3220 - val_loss: 3.3351 - val_acc: 0.3453\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 3.4329 - acc: 0.3298 - val_loss: 3.2410 - val_acc: 0.3542\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 3.3583 - acc: 0.3361 - val_loss: 3.1669 - val_acc: 0.3625\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 3.2834 - acc: 0.3435 - val_loss: 3.1102 - val_acc: 0.3679\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 3.2257 - acc: 0.3483 - val_loss: 3.0723 - val_acc: 0.3722\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 3.1713 - acc: 0.3546 - val_loss: 3.0289 - val_acc: 0.3785\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 2s 42ms/step - loss: 3.1190 - acc: 0.3588 - val_loss: 2.9937 - val_acc: 0.3820\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 3.0807 - acc: 0.3641 - val_loss: 2.9515 - val_acc: 0.3893\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 3.0403 - acc: 0.3699 - val_loss: 2.9224 - val_acc: 0.3961\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.9991 - acc: 0.3740 - val_loss: 2.9244 - val_acc: 0.3939\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 2.9671 - acc: 0.3782 - val_loss: 2.8649 - val_acc: 0.4007\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9324 - acc: 0.3822 - val_loss: 2.8664 - val_acc: 0.4040\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.9029 - acc: 0.3861 - val_loss: 2.8386 - val_acc: 0.4076\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.8706 - acc: 0.3907 - val_loss: 2.7948 - val_acc: 0.4138\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8428 - acc: 0.3912 - val_loss: 2.7734 - val_acc: 0.4151\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.8069 - acc: 0.3977 - val_loss: 2.7629 - val_acc: 0.4161\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7882 - acc: 0.4011 - val_loss: 2.7341 - val_acc: 0.4219\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.7599 - acc: 0.4055 - val_loss: 2.7357 - val_acc: 0.4227\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 2.7353 - acc: 0.4100 - val_loss: 2.7043 - val_acc: 0.4272\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.7109 - acc: 0.4123 - val_loss: 2.6992 - val_acc: 0.4282\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.6829 - acc: 0.4160 - val_loss: 2.6752 - val_acc: 0.4310\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6706 - acc: 0.4166 - val_loss: 2.6624 - val_acc: 0.4350\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.6437 - acc: 0.4205 - val_loss: 2.6578 - val_acc: 0.4368\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6243 - acc: 0.4244 - val_loss: 2.6478 - val_acc: 0.4392\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.5917 - acc: 0.4289 - val_loss: 2.6432 - val_acc: 0.4388\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5769 - acc: 0.4304 - val_loss: 2.6233 - val_acc: 0.4438\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5627 - acc: 0.4320 - val_loss: 2.6172 - val_acc: 0.4437\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.5406 - acc: 0.4366 - val_loss: 2.6116 - val_acc: 0.4455\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5251 - acc: 0.4392 - val_loss: 2.6012 - val_acc: 0.4460\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5071 - acc: 0.4428 - val_loss: 2.5937 - val_acc: 0.4497\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4899 - acc: 0.4433 - val_loss: 2.5829 - val_acc: 0.4498\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4673 - acc: 0.4461 - val_loss: 2.5788 - val_acc: 0.4520\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4489 - acc: 0.4511 - val_loss: 2.5751 - val_acc: 0.4549\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4330 - acc: 0.4539 - val_loss: 2.5736 - val_acc: 0.4539\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.4173 - acc: 0.4531 - val_loss: 2.5508 - val_acc: 0.4572\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.4001 - acc: 0.4567 - val_loss: 2.5458 - val_acc: 0.4609\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.3842 - acc: 0.4593 - val_loss: 2.5419 - val_acc: 0.4585\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.3641 - acc: 0.4620 - val_loss: 2.5451 - val_acc: 0.4605\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3508 - acc: 0.4636 - val_loss: 2.5373 - val_acc: 0.4610\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3418 - acc: 0.4655 - val_loss: 2.5174 - val_acc: 0.4638\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3217 - acc: 0.4677 - val_loss: 2.5084 - val_acc: 0.4646\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.2990 - acc: 0.4708 - val_loss: 2.5111 - val_acc: 0.4650\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2859 - acc: 0.4745 - val_loss: 2.5137 - val_acc: 0.4659\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2824 - acc: 0.4746 - val_loss: 2.5104 - val_acc: 0.4662\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2657 - acc: 0.4769 - val_loss: 2.5102 - val_acc: 0.4670\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.2428 - acc: 0.4785 - val_loss: 2.5053 - val_acc: 0.4689\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2305 - acc: 0.4798 - val_loss: 2.4986 - val_acc: 0.4681\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2173 - acc: 0.4843 - val_loss: 2.4952 - val_acc: 0.4699\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2068 - acc: 0.4858 - val_loss: 2.4920 - val_acc: 0.4716\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1858 - acc: 0.4868 - val_loss: 2.4961 - val_acc: 0.4703\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.1746 - acc: 0.4910 - val_loss: 2.4909 - val_acc: 0.4703\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1616 - acc: 0.4909 - val_loss: 2.4889 - val_acc: 0.4731\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1504 - acc: 0.4947 - val_loss: 2.5057 - val_acc: 0.4731\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1364 - acc: 0.4957 - val_loss: 2.4869 - val_acc: 0.4718\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1225 - acc: 0.4983 - val_loss: 2.4828 - val_acc: 0.4728\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1067 - acc: 0.5009 - val_loss: 2.4845 - val_acc: 0.4739\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.0954 - acc: 0.5023 - val_loss: 2.4805 - val_acc: 0.4737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0833 - acc: 0.5039 - val_loss: 2.4765 - val_acc: 0.4758\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0748 - acc: 0.5058 - val_loss: 2.4793 - val_acc: 0.4752\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.0614 - acc: 0.5076 - val_loss: 2.4699 - val_acc: 0.4773\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.0491 - acc: 0.5094 - val_loss: 2.4845 - val_acc: 0.4783\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.0340 - acc: 0.5116 - val_loss: 2.4793 - val_acc: 0.4776\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.0167 - acc: 0.5160 - val_loss: 2.4908 - val_acc: 0.4771\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.0153 - acc: 0.5167 - val_loss: 2.4871 - val_acc: 0.4773\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.9992 - acc: 0.5162 - val_loss: 2.4878 - val_acc: 0.4776\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9860 - acc: 0.5220 - val_loss: 2.4797 - val_acc: 0.4791\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1.9792 - acc: 0.5210 - val_loss: 2.4890 - val_acc: 0.4797\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9693 - acc: 0.5228 - val_loss: 2.4833 - val_acc: 0.4802\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9550 - acc: 0.5248 - val_loss: 2.4852 - val_acc: 0.4794\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.9333 - acc: 0.5292Restoring model weights from the end of the best epoch: 65.\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 1.9333 - acc: 0.5292 - val_loss: 2.4916 - val_acc: 0.4798\n",
      "Epoch 75: early stopping\n",
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.5\n",
      "Current Fold: 2/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n",
      "Epoch 1/200\n",
      "48/48 [==============================] - 29s 295ms/step - loss: 5.3037 - acc: 0.1504 - val_loss: 4.3344 - val_acc: 0.2308\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 4s 86ms/step - loss: 4.3787 - acc: 0.2282 - val_loss: 3.9977 - val_acc: 0.2619\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 3s 62ms/step - loss: 4.0815 - acc: 0.2640 - val_loss: 3.7242 - val_acc: 0.3020\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 2s 51ms/step - loss: 3.8520 - acc: 0.2929 - val_loss: 3.5330 - val_acc: 0.3251\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 3.6690 - acc: 0.3116 - val_loss: 3.3885 - val_acc: 0.3363\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 2s 48ms/step - loss: 3.5313 - acc: 0.3236 - val_loss: 3.2989 - val_acc: 0.3458\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 3.4331 - acc: 0.3309 - val_loss: 3.2159 - val_acc: 0.3536\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 3.3488 - acc: 0.3386 - val_loss: 3.1634 - val_acc: 0.3604\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 3.2780 - acc: 0.3441 - val_loss: 3.1246 - val_acc: 0.3645\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 2s 48ms/step - loss: 3.2172 - acc: 0.3502 - val_loss: 3.0547 - val_acc: 0.3737\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 3.1711 - acc: 0.3556 - val_loss: 3.0171 - val_acc: 0.3803\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 3.1221 - acc: 0.3597 - val_loss: 2.9930 - val_acc: 0.3816\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 3.0732 - acc: 0.3651 - val_loss: 2.9764 - val_acc: 0.3856\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 3.0333 - acc: 0.3715 - val_loss: 2.9366 - val_acc: 0.3930\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.9981 - acc: 0.3753 - val_loss: 2.8974 - val_acc: 0.3960\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.9563 - acc: 0.3804 - val_loss: 2.8818 - val_acc: 0.4025\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.9243 - acc: 0.3833 - val_loss: 2.8592 - val_acc: 0.4054\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8961 - acc: 0.3876 - val_loss: 2.8349 - val_acc: 0.4076\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8679 - acc: 0.3910 - val_loss: 2.8130 - val_acc: 0.4122\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8360 - acc: 0.3953 - val_loss: 2.7986 - val_acc: 0.4144\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.8102 - acc: 0.3967 - val_loss: 2.7692 - val_acc: 0.4190\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.7794 - acc: 0.4004 - val_loss: 2.7651 - val_acc: 0.4201\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.7535 - acc: 0.4064 - val_loss: 2.7552 - val_acc: 0.4229\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7357 - acc: 0.4091 - val_loss: 2.7340 - val_acc: 0.4258\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7109 - acc: 0.4108 - val_loss: 2.7185 - val_acc: 0.4284\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6855 - acc: 0.4147 - val_loss: 2.7011 - val_acc: 0.4298\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6586 - acc: 0.4194 - val_loss: 2.6912 - val_acc: 0.4326\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.6422 - acc: 0.4220 - val_loss: 2.6825 - val_acc: 0.4372\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 2.6179 - acc: 0.4252 - val_loss: 2.6642 - val_acc: 0.4365\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5982 - acc: 0.4259 - val_loss: 2.6550 - val_acc: 0.4391\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.5772 - acc: 0.4303 - val_loss: 2.6543 - val_acc: 0.4394\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.5573 - acc: 0.4336 - val_loss: 2.6495 - val_acc: 0.4410\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.5411 - acc: 0.4341 - val_loss: 2.6368 - val_acc: 0.4438\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.5216 - acc: 0.4385 - val_loss: 2.6205 - val_acc: 0.4451\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.5022 - acc: 0.4426 - val_loss: 2.6300 - val_acc: 0.4460\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 2.4853 - acc: 0.4429 - val_loss: 2.6261 - val_acc: 0.4459\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4684 - acc: 0.4473 - val_loss: 2.6039 - val_acc: 0.4512\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.4504 - acc: 0.4479 - val_loss: 2.5976 - val_acc: 0.4493\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.4267 - acc: 0.4516 - val_loss: 2.5847 - val_acc: 0.4517\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.4115 - acc: 0.4546 - val_loss: 2.5855 - val_acc: 0.4521\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.4016 - acc: 0.4559 - val_loss: 2.5813 - val_acc: 0.4561\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.3825 - acc: 0.4587 - val_loss: 2.5557 - val_acc: 0.4563\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3684 - acc: 0.4630 - val_loss: 2.5558 - val_acc: 0.4582\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3497 - acc: 0.4628 - val_loss: 2.5548 - val_acc: 0.4585\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.3325 - acc: 0.4675 - val_loss: 2.5579 - val_acc: 0.4594\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.3246 - acc: 0.4674 - val_loss: 2.5505 - val_acc: 0.4608\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3031 - acc: 0.4711 - val_loss: 2.5365 - val_acc: 0.4626\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2896 - acc: 0.4722 - val_loss: 2.5468 - val_acc: 0.4626\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2795 - acc: 0.4738 - val_loss: 2.5320 - val_acc: 0.4659\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2608 - acc: 0.4763 - val_loss: 2.5347 - val_acc: 0.4657\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2426 - acc: 0.4807 - val_loss: 2.5265 - val_acc: 0.4646\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.2277 - acc: 0.4811 - val_loss: 2.5234 - val_acc: 0.4696\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2169 - acc: 0.4848 - val_loss: 2.5274 - val_acc: 0.4675\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2043 - acc: 0.4857 - val_loss: 2.5355 - val_acc: 0.4661\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1910 - acc: 0.4876 - val_loss: 2.5176 - val_acc: 0.4698\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1718 - acc: 0.4904 - val_loss: 2.5153 - val_acc: 0.4710\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1618 - acc: 0.4930 - val_loss: 2.5156 - val_acc: 0.4717\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.1489 - acc: 0.4944 - val_loss: 2.5129 - val_acc: 0.4705\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1353 - acc: 0.4969 - val_loss: 2.5068 - val_acc: 0.4730\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1236 - acc: 0.4991 - val_loss: 2.5157 - val_acc: 0.4733\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1087 - acc: 0.5008 - val_loss: 2.5153 - val_acc: 0.4732\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0981 - acc: 0.5022 - val_loss: 2.5210 - val_acc: 0.4743\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0842 - acc: 0.5042 - val_loss: 2.5109 - val_acc: 0.4722\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0803 - acc: 0.5057 - val_loss: 2.5093 - val_acc: 0.4739\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0671 - acc: 0.5056 - val_loss: 2.5072 - val_acc: 0.4764\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0557 - acc: 0.5116 - val_loss: 2.4958 - val_acc: 0.4758\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0357 - acc: 0.5131 - val_loss: 2.4990 - val_acc: 0.4754\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0301 - acc: 0.5136 - val_loss: 2.5069 - val_acc: 0.4740\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0126 - acc: 0.5153 - val_loss: 2.5053 - val_acc: 0.4773\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.0065 - acc: 0.5174 - val_loss: 2.5148 - val_acc: 0.4758\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 1.9898 - acc: 0.5190 - val_loss: 2.4969 - val_acc: 0.4789\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9758 - acc: 0.5206 - val_loss: 2.5127 - val_acc: 0.4806\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9621 - acc: 0.5254 - val_loss: 2.5119 - val_acc: 0.4775\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 1.9550 - acc: 0.5256 - val_loss: 2.5036 - val_acc: 0.4780\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9457 - acc: 0.5249 - val_loss: 2.4960 - val_acc: 0.4782\n",
      "Epoch 76/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.9335 - acc: 0.5276Restoring model weights from the end of the best epoch: 66.\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 1.9335 - acc: 0.5276 - val_loss: 2.5032 - val_acc: 0.4785\n",
      "Epoch 76: early stopping\n",
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.5\n",
      "Current Fold: 3/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n",
      "Epoch 1/200\n",
      "48/48 [==============================] - 29s 298ms/step - loss: 5.2436 - acc: 0.1520 - val_loss: 4.3717 - val_acc: 0.2270\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 4s 85ms/step - loss: 4.3602 - acc: 0.2302 - val_loss: 4.0229 - val_acc: 0.2683\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 3s 63ms/step - loss: 4.0579 - acc: 0.2668 - val_loss: 3.7547 - val_acc: 0.3003\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 3s 59ms/step - loss: 3.8298 - acc: 0.2923 - val_loss: 3.5731 - val_acc: 0.3199\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 2s 41ms/step - loss: 3.6554 - acc: 0.3101 - val_loss: 3.4318 - val_acc: 0.3333\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 3.5255 - acc: 0.3225 - val_loss: 3.3191 - val_acc: 0.3460\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 2s 43ms/step - loss: 3.4252 - acc: 0.3310 - val_loss: 3.2687 - val_acc: 0.3488\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 3.3468 - acc: 0.3377 - val_loss: 3.1822 - val_acc: 0.3579\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 3.2809 - acc: 0.3436 - val_loss: 3.1281 - val_acc: 0.3639\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 3.2212 - acc: 0.3510 - val_loss: 3.0712 - val_acc: 0.3698\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 3.1661 - acc: 0.3556 - val_loss: 3.0292 - val_acc: 0.3792\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 3.1255 - acc: 0.3594 - val_loss: 3.0044 - val_acc: 0.3827\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 3.0819 - acc: 0.3635 - val_loss: 2.9751 - val_acc: 0.3873\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 3.0421 - acc: 0.3697 - val_loss: 2.9305 - val_acc: 0.3925\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 3.0043 - acc: 0.3729 - val_loss: 2.9432 - val_acc: 0.3906\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.9695 - acc: 0.3766 - val_loss: 2.8756 - val_acc: 0.3997\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9331 - acc: 0.3836 - val_loss: 2.8576 - val_acc: 0.4048\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9015 - acc: 0.3873 - val_loss: 2.8333 - val_acc: 0.4088\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 2.8722 - acc: 0.3907 - val_loss: 2.8143 - val_acc: 0.4097\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8396 - acc: 0.3967 - val_loss: 2.7966 - val_acc: 0.4138\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8119 - acc: 0.3992 - val_loss: 2.7774 - val_acc: 0.4164\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7859 - acc: 0.4024 - val_loss: 2.7688 - val_acc: 0.4192\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 2.7580 - acc: 0.4067 - val_loss: 2.7345 - val_acc: 0.4232\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7311 - acc: 0.4114 - val_loss: 2.7272 - val_acc: 0.4253\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7070 - acc: 0.4142 - val_loss: 2.7054 - val_acc: 0.4293\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.6871 - acc: 0.4175 - val_loss: 2.6854 - val_acc: 0.4301\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 2.6654 - acc: 0.4211 - val_loss: 2.6883 - val_acc: 0.4329\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.6440 - acc: 0.4241 - val_loss: 2.6733 - val_acc: 0.4359\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6186 - acc: 0.4271 - val_loss: 2.6828 - val_acc: 0.4356\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 2s 33ms/step - loss: 2.6019 - acc: 0.4280 - val_loss: 2.6475 - val_acc: 0.4399\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5770 - acc: 0.4319 - val_loss: 2.6360 - val_acc: 0.4410\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5598 - acc: 0.4348 - val_loss: 2.6326 - val_acc: 0.4437\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 2.5411 - acc: 0.4374 - val_loss: 2.6116 - val_acc: 0.4456\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.5177 - acc: 0.4421 - val_loss: 2.6092 - val_acc: 0.4462\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5023 - acc: 0.4430 - val_loss: 2.5947 - val_acc: 0.4475\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 2.4843 - acc: 0.4432 - val_loss: 2.5840 - val_acc: 0.4505\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.4657 - acc: 0.4474 - val_loss: 2.5794 - val_acc: 0.4522\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.4499 - acc: 0.4520 - val_loss: 2.5780 - val_acc: 0.4517\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4280 - acc: 0.4543 - val_loss: 2.5773 - val_acc: 0.4530\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4145 - acc: 0.4554 - val_loss: 2.5588 - val_acc: 0.4538\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3975 - acc: 0.4621 - val_loss: 2.5629 - val_acc: 0.4553\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3793 - acc: 0.4629 - val_loss: 2.5534 - val_acc: 0.4582\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.3665 - acc: 0.4643 - val_loss: 2.5509 - val_acc: 0.4574\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3459 - acc: 0.4657 - val_loss: 2.5428 - val_acc: 0.4592\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.3300 - acc: 0.4700 - val_loss: 2.5395 - val_acc: 0.4617\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3182 - acc: 0.4716 - val_loss: 2.5371 - val_acc: 0.4604\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3047 - acc: 0.4721 - val_loss: 2.5179 - val_acc: 0.4628\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2853 - acc: 0.4757 - val_loss: 2.5288 - val_acc: 0.4622\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2698 - acc: 0.4784 - val_loss: 2.5167 - val_acc: 0.4642\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.2576 - acc: 0.4792 - val_loss: 2.4987 - val_acc: 0.4673\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.2458 - acc: 0.4823 - val_loss: 2.5132 - val_acc: 0.4658\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.2319 - acc: 0.4831 - val_loss: 2.5114 - val_acc: 0.4675\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.2185 - acc: 0.4863 - val_loss: 2.5034 - val_acc: 0.4689\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.2040 - acc: 0.4867 - val_loss: 2.4869 - val_acc: 0.4700\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1864 - acc: 0.4915 - val_loss: 2.4920 - val_acc: 0.4673\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1716 - acc: 0.4916 - val_loss: 2.4946 - val_acc: 0.4703\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1597 - acc: 0.4933 - val_loss: 2.4908 - val_acc: 0.4707\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1520 - acc: 0.4953 - val_loss: 2.4952 - val_acc: 0.4710\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1364 - acc: 0.4989 - val_loss: 2.4902 - val_acc: 0.4715\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1229 - acc: 0.5004 - val_loss: 2.4890 - val_acc: 0.4719\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1076 - acc: 0.5036 - val_loss: 2.4907 - val_acc: 0.4725\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1003 - acc: 0.5059 - val_loss: 2.4835 - val_acc: 0.4738\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0861 - acc: 0.5066 - val_loss: 2.4889 - val_acc: 0.4742\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0629 - acc: 0.5091 - val_loss: 2.4772 - val_acc: 0.4753\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0586 - acc: 0.5103 - val_loss: 2.4814 - val_acc: 0.4745\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0466 - acc: 0.5131 - val_loss: 2.4691 - val_acc: 0.4752\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0323 - acc: 0.5174 - val_loss: 2.4829 - val_acc: 0.4757\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.0257 - acc: 0.5163 - val_loss: 2.4727 - val_acc: 0.4754\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0083 - acc: 0.5181 - val_loss: 2.4846 - val_acc: 0.4754\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9974 - acc: 0.5206 - val_loss: 2.4943 - val_acc: 0.4771\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9835 - acc: 0.5229 - val_loss: 2.4882 - val_acc: 0.4782\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9712 - acc: 0.5255 - val_loss: 2.4827 - val_acc: 0.4741\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9624 - acc: 0.5260 - val_loss: 2.4802 - val_acc: 0.4765\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9509 - acc: 0.5287 - val_loss: 2.4966 - val_acc: 0.4772\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9337 - acc: 0.5303 - val_loss: 2.4820 - val_acc: 0.4787\n",
      "Epoch 76/200\n",
      "47/48 [============================>.] - ETA: 0s - loss: 1.9273 - acc: 0.5294Restoring model weights from the end of the best epoch: 66.\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9279 - acc: 0.5294 - val_loss: 2.4848 - val_acc: 0.4780\n",
      "Epoch 76: early stopping\n",
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.5\n",
      "Current Fold: 4/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n",
      "Epoch 1/200\n",
      "48/48 [==============================] - 28s 277ms/step - loss: 5.2441 - acc: 0.1454 - val_loss: 4.3415 - val_acc: 0.2225\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 4s 81ms/step - loss: 4.3566 - acc: 0.2312 - val_loss: 4.0170 - val_acc: 0.2619\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 3s 65ms/step - loss: 4.0632 - acc: 0.2693 - val_loss: 3.7625 - val_acc: 0.3011\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 3s 59ms/step - loss: 3.8356 - acc: 0.2956 - val_loss: 3.5777 - val_acc: 0.3220\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 3.6624 - acc: 0.3127 - val_loss: 3.4299 - val_acc: 0.3376\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 3.5241 - acc: 0.3220 - val_loss: 3.3434 - val_acc: 0.3434\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 3.4196 - acc: 0.3328 - val_loss: 3.2523 - val_acc: 0.3512\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 2s 41ms/step - loss: 3.3421 - acc: 0.3392 - val_loss: 3.1952 - val_acc: 0.3571\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 3.2698 - acc: 0.3456 - val_loss: 3.1483 - val_acc: 0.3633\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 3.2034 - acc: 0.3531 - val_loss: 3.0838 - val_acc: 0.3703\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 3.1615 - acc: 0.3549 - val_loss: 3.0502 - val_acc: 0.3760\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 3.1122 - acc: 0.3615 - val_loss: 3.0165 - val_acc: 0.3807\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 31ms/step - loss: 3.0644 - acc: 0.3674 - val_loss: 3.0024 - val_acc: 0.3821\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 3.0257 - acc: 0.3721 - val_loss: 2.9527 - val_acc: 0.3882\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9851 - acc: 0.3776 - val_loss: 2.9210 - val_acc: 0.3925\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9520 - acc: 0.3813 - val_loss: 2.9120 - val_acc: 0.3962\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9155 - acc: 0.3834 - val_loss: 2.8953 - val_acc: 0.3984\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.8858 - acc: 0.3871 - val_loss: 2.8591 - val_acc: 0.4040\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.8574 - acc: 0.3898 - val_loss: 2.8599 - val_acc: 0.4050\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8266 - acc: 0.3948 - val_loss: 2.8177 - val_acc: 0.4097\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 2.8002 - acc: 0.3984 - val_loss: 2.8001 - val_acc: 0.4131\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 2.7706 - acc: 0.4029 - val_loss: 2.7778 - val_acc: 0.4165\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7537 - acc: 0.4041 - val_loss: 2.7818 - val_acc: 0.4170\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.7177 - acc: 0.4102 - val_loss: 2.7491 - val_acc: 0.4196\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6971 - acc: 0.4129 - val_loss: 2.7375 - val_acc: 0.4254\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 2.6760 - acc: 0.4163 - val_loss: 2.7296 - val_acc: 0.4253\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.6509 - acc: 0.4188 - val_loss: 2.7192 - val_acc: 0.4288\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.6300 - acc: 0.4236 - val_loss: 2.6983 - val_acc: 0.4322\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.6066 - acc: 0.4242 - val_loss: 2.7077 - val_acc: 0.4307\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.5839 - acc: 0.4290 - val_loss: 2.6755 - val_acc: 0.4359\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5691 - acc: 0.4301 - val_loss: 2.6672 - val_acc: 0.4348\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.5458 - acc: 0.4340 - val_loss: 2.6643 - val_acc: 0.4373\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.5254 - acc: 0.4366 - val_loss: 2.6556 - val_acc: 0.4406\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.5058 - acc: 0.4408 - val_loss: 2.6484 - val_acc: 0.4427\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.4931 - acc: 0.4406 - val_loss: 2.6245 - val_acc: 0.4447\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4749 - acc: 0.4433 - val_loss: 2.6347 - val_acc: 0.4454\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 2.4570 - acc: 0.4463 - val_loss: 2.6204 - val_acc: 0.4470\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.4356 - acc: 0.4502 - val_loss: 2.6123 - val_acc: 0.4480\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.4213 - acc: 0.4524 - val_loss: 2.6157 - val_acc: 0.4477\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.3994 - acc: 0.4556 - val_loss: 2.6015 - val_acc: 0.4530\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3913 - acc: 0.4570 - val_loss: 2.5928 - val_acc: 0.4539\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3681 - acc: 0.4623 - val_loss: 2.5721 - val_acc: 0.4551\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.3462 - acc: 0.4634 - val_loss: 2.5960 - val_acc: 0.4544\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3368 - acc: 0.4678 - val_loss: 2.5722 - val_acc: 0.4557\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3217 - acc: 0.4669 - val_loss: 2.5647 - val_acc: 0.4581\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3154 - acc: 0.4662 - val_loss: 2.5578 - val_acc: 0.4602\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2874 - acc: 0.4725 - val_loss: 2.5726 - val_acc: 0.4589\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.2766 - acc: 0.4740 - val_loss: 2.5552 - val_acc: 0.4597\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.2607 - acc: 0.4766 - val_loss: 2.5621 - val_acc: 0.4618\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2479 - acc: 0.4781 - val_loss: 2.5575 - val_acc: 0.4614\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2316 - acc: 0.4805 - val_loss: 2.5421 - val_acc: 0.4630\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2182 - acc: 0.4841 - val_loss: 2.5433 - val_acc: 0.4643\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2041 - acc: 0.4872 - val_loss: 2.5438 - val_acc: 0.4656\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1921 - acc: 0.4892 - val_loss: 2.5357 - val_acc: 0.4673\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.1801 - acc: 0.4910 - val_loss: 2.5377 - val_acc: 0.4675\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1611 - acc: 0.4929 - val_loss: 2.5346 - val_acc: 0.4663\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1557 - acc: 0.4931 - val_loss: 2.5327 - val_acc: 0.4658\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1403 - acc: 0.4950 - val_loss: 2.5350 - val_acc: 0.4666\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.1259 - acc: 0.4974 - val_loss: 2.5350 - val_acc: 0.4683\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1087 - acc: 0.5019 - val_loss: 2.5274 - val_acc: 0.4671\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0959 - acc: 0.5019 - val_loss: 2.5252 - val_acc: 0.4694\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0861 - acc: 0.5055 - val_loss: 2.5235 - val_acc: 0.4681\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0766 - acc: 0.5051 - val_loss: 2.5283 - val_acc: 0.4688\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.0594 - acc: 0.5093 - val_loss: 2.5268 - val_acc: 0.4715\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0525 - acc: 0.5101 - val_loss: 2.5356 - val_acc: 0.4683\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.0401 - acc: 0.5110 - val_loss: 2.5314 - val_acc: 0.4698\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0213 - acc: 0.5147 - val_loss: 2.5325 - val_acc: 0.4704\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0079 - acc: 0.5188 - val_loss: 2.5316 - val_acc: 0.4720\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0008 - acc: 0.5196 - val_loss: 2.5258 - val_acc: 0.4725\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9845 - acc: 0.5218 - val_loss: 2.5213 - val_acc: 0.4734\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9789 - acc: 0.5214 - val_loss: 2.5300 - val_acc: 0.4718\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9632 - acc: 0.5259 - val_loss: 2.5277 - val_acc: 0.4727\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9443 - acc: 0.5272 - val_loss: 2.5347 - val_acc: 0.4720\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9413 - acc: 0.5279 - val_loss: 2.5368 - val_acc: 0.4745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9236 - acc: 0.5299 - val_loss: 2.5440 - val_acc: 0.4749\n",
      "Epoch 76/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9203 - acc: 0.5317 - val_loss: 2.5334 - val_acc: 0.4775\n",
      "Epoch 77/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9097 - acc: 0.5339 - val_loss: 2.5308 - val_acc: 0.4767\n",
      "Epoch 78/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.8987 - acc: 0.5364 - val_loss: 2.5351 - val_acc: 0.4777\n",
      "Epoch 79/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.8866 - acc: 0.5363 - val_loss: 2.5457 - val_acc: 0.4779\n",
      "Epoch 80/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.8725 - acc: 0.5402Restoring model weights from the end of the best epoch: 70.\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.8725 - acc: 0.5402 - val_loss: 2.5407 - val_acc: 0.4763\n",
      "Epoch 80: early stopping\n",
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.5\n",
      "Current Fold: 5/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n",
      "Epoch 1/200\n",
      "48/48 [==============================] - 28s 284ms/step - loss: 5.2521 - acc: 0.1467 - val_loss: 4.3160 - val_acc: 0.2261\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 4s 77ms/step - loss: 4.3772 - acc: 0.2260 - val_loss: 3.9965 - val_acc: 0.2642\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 3s 53ms/step - loss: 4.0722 - acc: 0.2670 - val_loss: 3.7158 - val_acc: 0.3049\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 2s 47ms/step - loss: 3.8394 - acc: 0.2949 - val_loss: 3.5480 - val_acc: 0.3195\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 2s 52ms/step - loss: 3.6635 - acc: 0.3127 - val_loss: 3.4208 - val_acc: 0.3301\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 3.5308 - acc: 0.3250 - val_loss: 3.3198 - val_acc: 0.3410\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 3.4306 - acc: 0.3320 - val_loss: 3.2494 - val_acc: 0.3481\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 3.3502 - acc: 0.3382 - val_loss: 3.1861 - val_acc: 0.3540\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 3.2846 - acc: 0.3461 - val_loss: 3.1570 - val_acc: 0.3578\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 3.2240 - acc: 0.3506 - val_loss: 3.0799 - val_acc: 0.3655\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 3.1638 - acc: 0.3557 - val_loss: 3.0452 - val_acc: 0.3711\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 3.1222 - acc: 0.3612 - val_loss: 3.0499 - val_acc: 0.3697\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 2s 42ms/step - loss: 3.0743 - acc: 0.3671 - val_loss: 2.9763 - val_acc: 0.3789\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 3.0340 - acc: 0.3726 - val_loss: 2.9458 - val_acc: 0.3837\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9893 - acc: 0.3787 - val_loss: 2.9287 - val_acc: 0.3848\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.9599 - acc: 0.3800 - val_loss: 2.8994 - val_acc: 0.3911\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.9289 - acc: 0.3851 - val_loss: 2.8705 - val_acc: 0.3936\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 2.8926 - acc: 0.3899 - val_loss: 2.8415 - val_acc: 0.4009\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.8609 - acc: 0.3948 - val_loss: 2.8292 - val_acc: 0.4009\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.8352 - acc: 0.3975 - val_loss: 2.8294 - val_acc: 0.4023\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.8082 - acc: 0.3994 - val_loss: 2.8006 - val_acc: 0.4070\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 2.7807 - acc: 0.4043 - val_loss: 2.7769 - val_acc: 0.4122\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.7542 - acc: 0.4074 - val_loss: 2.7711 - val_acc: 0.4113\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.7294 - acc: 0.4104 - val_loss: 2.7286 - val_acc: 0.4180\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 2.7008 - acc: 0.4146 - val_loss: 2.7269 - val_acc: 0.4210\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.6846 - acc: 0.4161 - val_loss: 2.7087 - val_acc: 0.4225\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.6635 - acc: 0.4214 - val_loss: 2.6979 - val_acc: 0.4244\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.6402 - acc: 0.4251 - val_loss: 2.6829 - val_acc: 0.4268\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.6185 - acc: 0.4239 - val_loss: 2.6774 - val_acc: 0.4278\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.5940 - acc: 0.4312 - val_loss: 2.6708 - val_acc: 0.4291\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.5794 - acc: 0.4322 - val_loss: 2.6497 - val_acc: 0.4331\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.5599 - acc: 0.4355 - val_loss: 2.6498 - val_acc: 0.4319\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.5367 - acc: 0.4380 - val_loss: 2.6288 - val_acc: 0.4336\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.5183 - acc: 0.4405 - val_loss: 2.6352 - val_acc: 0.4353\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.4983 - acc: 0.4443 - val_loss: 2.6189 - val_acc: 0.4357\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4833 - acc: 0.4460 - val_loss: 2.6231 - val_acc: 0.4385\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.4615 - acc: 0.4503 - val_loss: 2.6170 - val_acc: 0.4420\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4475 - acc: 0.4510 - val_loss: 2.5986 - val_acc: 0.4426\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4363 - acc: 0.4525 - val_loss: 2.5869 - val_acc: 0.4432\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.4128 - acc: 0.4559 - val_loss: 2.5878 - val_acc: 0.4442\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 2.3957 - acc: 0.4602 - val_loss: 2.5772 - val_acc: 0.4454\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.3842 - acc: 0.4614 - val_loss: 2.5739 - val_acc: 0.4465\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.3742 - acc: 0.4623 - val_loss: 2.5699 - val_acc: 0.4492\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.3522 - acc: 0.4654 - val_loss: 2.5689 - val_acc: 0.4501\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.3321 - acc: 0.4693 - val_loss: 2.5584 - val_acc: 0.4501\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.3162 - acc: 0.4718 - val_loss: 2.5459 - val_acc: 0.4513\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.2972 - acc: 0.4740 - val_loss: 2.5477 - val_acc: 0.4520\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2894 - acc: 0.4740 - val_loss: 2.5469 - val_acc: 0.4551\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.2747 - acc: 0.4765 - val_loss: 2.5366 - val_acc: 0.4548\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.2612 - acc: 0.4788 - val_loss: 2.5301 - val_acc: 0.4549\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2393 - acc: 0.4821 - val_loss: 2.5334 - val_acc: 0.4556\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.2314 - acc: 0.4843 - val_loss: 2.5312 - val_acc: 0.4551\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.2215 - acc: 0.4856 - val_loss: 2.5219 - val_acc: 0.4587\n",
      "Epoch 54/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 26ms/step - loss: 2.1983 - acc: 0.4909 - val_loss: 2.5274 - val_acc: 0.4590\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 2.1876 - acc: 0.4901 - val_loss: 2.5253 - val_acc: 0.4571\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1727 - acc: 0.4924 - val_loss: 2.5161 - val_acc: 0.4581\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.1602 - acc: 0.4941 - val_loss: 2.5086 - val_acc: 0.4608\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.1493 - acc: 0.4975 - val_loss: 2.5029 - val_acc: 0.4634\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.1355 - acc: 0.4983 - val_loss: 2.5109 - val_acc: 0.4628\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 2.1202 - acc: 0.5006 - val_loss: 2.5156 - val_acc: 0.4621\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 2.1081 - acc: 0.5039 - val_loss: 2.5096 - val_acc: 0.4635\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0929 - acc: 0.5062 - val_loss: 2.5086 - val_acc: 0.4647\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.0825 - acc: 0.5065 - val_loss: 2.4982 - val_acc: 0.4644\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 2.0732 - acc: 0.5084 - val_loss: 2.4970 - val_acc: 0.4645\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 2.0596 - acc: 0.5116 - val_loss: 2.5028 - val_acc: 0.4659\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.0490 - acc: 0.5118 - val_loss: 2.5006 - val_acc: 0.4662\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 2.0327 - acc: 0.5150 - val_loss: 2.5010 - val_acc: 0.4645\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0209 - acc: 0.5165 - val_loss: 2.5044 - val_acc: 0.4652\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 2.0157 - acc: 0.5165 - val_loss: 2.4986 - val_acc: 0.4653\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 1.9921 - acc: 0.5239 - val_loss: 2.5165 - val_acc: 0.4669\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9899 - acc: 0.5217 - val_loss: 2.4997 - val_acc: 0.4679\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9782 - acc: 0.5235 - val_loss: 2.4982 - val_acc: 0.4677\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 1.9644 - acc: 0.5246 - val_loss: 2.5019 - val_acc: 0.4664\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 1.9513 - acc: 0.5286 - val_loss: 2.4957 - val_acc: 0.4682\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 1.9454 - acc: 0.5293 - val_loss: 2.5040 - val_acc: 0.4675\n",
      "Epoch 76/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.9337 - acc: 0.5320 - val_loss: 2.5051 - val_acc: 0.4678\n",
      "Epoch 77/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9203 - acc: 0.5340 - val_loss: 2.4976 - val_acc: 0.4697\n",
      "Epoch 78/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.9111 - acc: 0.5350 - val_loss: 2.5091 - val_acc: 0.4680\n",
      "Epoch 79/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.8941 - acc: 0.5376 - val_loss: 2.4979 - val_acc: 0.4694\n",
      "Epoch 80/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.8808 - acc: 0.5409 - val_loss: 2.5004 - val_acc: 0.4720\n",
      "Epoch 81/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 1.8797 - acc: 0.5402 - val_loss: 2.5054 - val_acc: 0.4692\n",
      "Epoch 82/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.8662 - acc: 0.5424 - val_loss: 2.5044 - val_acc: 0.4698\n",
      "Epoch 83/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 1.8523 - acc: 0.5446 - val_loss: 2.5031 - val_acc: 0.4712\n",
      "Epoch 84/200\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.8427 - acc: 0.5472Restoring model weights from the end of the best epoch: 74.\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 1.8427 - acc: 0.5472 - val_loss: 2.5057 - val_acc: 0.4706\n",
      "Epoch 84: early stopping\n",
      "[2.469913959503174, 2.4958345890045166, 2.469052791595459, 2.5213451385498047, 2.4956765174865723]\n",
      "[65, 66, 66, 70, 74]\n",
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.6\n",
      "Current Fold: 1/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n",
      "Epoch 1/200\n",
      "48/48 [==============================] - 27s 248ms/step - loss: 5.4565 - acc: 0.1334 - val_loss: 4.4859 - val_acc: 0.2073\n",
      "Epoch 2/200\n",
      "14/48 [=======>......................] - ETA: 3s - loss: 4.6107 - acc: 0.1942"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44872/626946897.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Metrika nad kojom se vrsi selekcija je smooth BLEU4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0maverage_bleu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_bleu4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44872/4102772494.py\u001b[0m in \u001b[0;36mcv_grid_search\u001b[0;34m(df, dropout_rates, latent_dims, epochs, learning_rate, folds)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mbest_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu4s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu3s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44872/758511062.py\u001b[0m in \u001b[0;36mcv_evaluate\u001b[0;34m(train_val_data, df_folds, folds, epochs, batch_size, learning_rate, latent_dim, dropout_rate, embedding_learning_rate)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current Learning Rate Multiplier: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_learning_rate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mbest_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44872/2215398689.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_data, val_data, epochs, batch_size, learning_rate, latent_dim, dropout_rate, embedding_learning_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m      \u001b[0mmodel_gru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m      \u001b[0;31m#print('Model compiled.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m      \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output_data_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m      \u001b[0;31m#print('Model fit.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m      \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Izvrsava se grid search nad hiperparametrima, i radi se unakrsna validacija za evaluaciju performansi\n",
    "#Metrika nad kojom se vrsi selekcija je smooth BLEU4\n",
    "\n",
    "metrics = cv_grid_search(df_train_val, dropout_rates, latent_dims, epochs = 200, learning_rate = learning_rate, folds = folds)\n",
    "average_bleu4 = np.mean(metrics['smooth_bleu4'], axis = -1)\n",
    "\n",
    "plt.title('Average smooth BLEU4, crossvalidated')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.xticks(range(len(dropout_rates)), dropout_rates)\n",
    "plt.ylabel('Latent Dim')\n",
    "plt.yticks(range(len(latent_dims)),latent_dims)\n",
    "plt.imshow(average_bleu4)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config_index = np.unravel_index(np.argmax(average_bleu4), average_bleu4.shape)\n",
    "best_latent_dim = latent_dims[best_config_index[0]]\n",
    "best_dropout_rate = dropout_rates[best_config_index[1]]\n",
    "print(\"Best latent dimension: \", best_latent_dim)\n",
    "print(\"Best dropout rate: \", best_dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388532a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_multipliers = [0.03, 0.1, 0.3, 1, 3, 10, 30] #Treba probati i vece vrednosti, posto 3 ispada optimalno (ovo je neocekivano)\n",
    "#learning_rate_multipliers = [100]\n",
    "best_epoch_array = []\n",
    "bleu4_array = []\n",
    "for i in range(len(learning_rate_multipliers)):\n",
    "    df_np = df_train_val.to_numpy()\n",
    "    np.random.shuffle(df_np)\n",
    "    total_size = df_np.shape[0]\n",
    "    fold_size = total_size/folds\n",
    "    df_folds = [df_np[int(i*fold_size):int((i+1)*fold_size),] for i in range(folds)]\n",
    "    best_epochs, _, _, smooth_bleu4s, _, _, _ = cv_evaluate(df_train_val, df_folds, folds = folds, epochs = 200, learning_rate = learning_rate, embedding_learning_rate = learning_rate_multipliers[i]*learning_rate, latent_dim = best_latent_dim, dropout_rate = best_dropout_rate)\n",
    "    best_epoch_array.append(best_epochs)\n",
    "    bleu4_array.append(smooth_bleu4s)\n",
    "\n",
    "bleu4_array = np.array(bleu4_array)\n",
    "best_epoch_array = np.array(best_epoch_array)\n",
    "best_multiplier_index = np.argmax(np.mean(bleu4_array, -1))\n",
    "bleu4_avg = np.mean(bleu4_array[best_multiplier_index])\n",
    "print(\"Average BLEU4(smooth) on validation: \", bleu4_avg)\n",
    "best_multiplier = learning_rate_multipliers[best_multiplier_index]\n",
    "print(\"Best Multiplier for Embedding Learning Rates:\", best_multiplier)\n",
    "best_multiplier_epochs = best_epoch_array[best_multiplier_index]\n",
    "print('Epochs to convergence on all folds: ', best_multiplier_epochs) #Gledamo koliko je epoha bilo potrebno do konvergencije\n",
    "epoch_avg = np.mean(best_multiplier_epochs)#Prosek koristimo za broj epoha treniranja modela na trening i validacionom skupu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ff800",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Average smooth BLEU4 for different embedding learning rates')\n",
    "plt.xlabel('Learning Rate Multiplier')\n",
    "plt.yticks([])\n",
    "plt.xticks(range(len(learning_rate_multipliers)), learning_rate_multipliers)\n",
    "plt.imshow(np.mean(bleu4_array, axis = 1).reshape(1,-1))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48727f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val_np = df_train_val.to_numpy()\n",
    "np.random.shuffle(df_train_val_np)\n",
    "split_size = 300 #Mozda treba vise\n",
    "train_data = df_train_val_np[:train_size + val_size - split_size,]\n",
    "val_data = df_train_val_np[train_size + val_size - split_size:,]\n",
    "input_texts, target_texts = clean_texts(train_data[:,1], train_data[:,0])\n",
    "input_word_index, target_word_index, max_input_seq_len, max_target_seq_len = analyse_texts(input_texts, target_texts)\n",
    "input_pad_len = 80\n",
    "target_pad_len = 60\n",
    "num_input_words = len(input_word_index) - 1\n",
    "num_target_words = len(target_word_index) - 1\n",
    "inverted_input_word_index = {value: key for key,value in input_word_index.items()}\n",
    "inverted_target_word_index = {value: key for (key,value) in target_word_index.items()}\n",
    "input_embedding_matrix, target_embedding_matrix = load_embedding_data_get_matrices(inverted_input_word_index, inverted_target_word_index)\n",
    "encoder_input_data, decoder_input_data, decoder_output_data = create_model_data(input_texts, target_texts, input_word_index, target_word_index, input_pad_len, target_pad_len)\n",
    "\n",
    "input_texts_val, target_texts_val = clean_texts(val_data[:,1], val_data[:,0])\n",
    "encoder_input_data_val, decoder_input_data_val, decoder_output_data_val = create_model_data(input_texts_val, target_texts_val, input_word_index, target_word_index, input_pad_len, target_pad_len)\n",
    "\n",
    "input_texts_test, target_texts_test = clean_texts_df(df_test)\n",
    "encoder_input_data_test, decoder_input_data_test, decoder_output_data_test = create_model_data(input_texts_test, target_texts_test, input_word_index, target_word_index, input_pad_len, target_pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_evaluation = GRU_Translation_Model(num_input_words, num_target_words, input_embedding_matrix, target_embedding_matrix)\n",
    "\n",
    "other_layers = model_for_evaluation.layers[0].layers + model_for_evaluation.layers[1].layers #Mora da se prilagodi za transformer\n",
    "embedding_layers = [other_layers.pop(2), other_layers.pop(-9)] #Paznja! Mora se prilagoditi svaki put kad se model menja\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers = [(Adam(learning_rate), other_layers), (Adam(best_multiplier*learning_rate), embedding_layers)])\n",
    "model_for_evaluation.compile(optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "#stajalo start_from_epoch = int(epoch_avg*0.7), ali iz nekog razloga ne prepoznaje argument\n",
    "early_stopping_safe = EarlyStopping(patience = 20, restore_best_weights = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n",
    "history = model_for_evaluation.fit([encoder_input_data, decoder_input_data], decoder_output_data, validation_data = ([encoder_input_data_val, decoder_input_data_val], decoder_output_data_val), epochs = 200, batch_size = 128, verbose = 1, callbacks = [early_stopping_safe])\n",
    "model_for_evaluation.summary()\n",
    "epoch_counter = range(len(history.history['loss']))\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "ax1.plot(epoch_counter, history.history['loss'], label = 'Train Loss')\n",
    "ax1.plot(epoch_counter, history.history['val_loss'], label = 'Validation Loss', linestyle = 'dashed')\n",
    "ax1.legend()\n",
    "ax2.plot(epoch_counter, history.history['acc'], label = 'Train accuracy')\n",
    "ax2.plot(epoch_counter, history.history['val_acc'], label = 'Validation Accuracy', linestyle = 'dashed')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1 = evaluate(model_for_evaluation, input_texts_test, target_texts_test, input_word_index, target_word_index, inverted_target_word_index, input_pad_len, target_pad_len)\n",
    "print('Results on test data:')\n",
    "print('Word Error Rate: ', wer) #76\n",
    "print('BLEU4(smooth): ', smooth_bleu4) #11.7\n",
    "print('BLEU3(smooth): ', smooth_bleu3) #17.1\n",
    "print('BLEU2(smooth): ', smooth_bleu2) #25.2\n",
    "print('BLEU1(smooth): ', smooth_bleu1) #36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b9d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
