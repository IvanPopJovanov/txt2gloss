{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c8b3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 22:06:06.874941: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-15 22:06:06.902808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-15 22:06:07.301353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.utils import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from tensorflow.keras import backend \n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from keras_nlp.layers import SinePositionEncoding\n",
    "\n",
    "import pydot\n",
    "\n",
    "from funkcije import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e94962",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model_weights_{epoch}.h5', save_best_only=False, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "#Ckeckpoint se vise ne koristi\n",
    "early_stopping = EarlyStopping(patience = 10, restore_best_weights = True, monitor = 'val_loss', mode = 'min', verbose = 1)\n",
    "\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8993ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Translation_Model(Model):\n",
    "    def __init__(self, num_input_words, num_target_words, input_embedding_matrix, target_embedding_matrix, latent_dim = 256, dropout_rate = 0.5, custom_dropout_rate = 0.05):\n",
    "        super(Transformer_Translation_Model, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.custom_dropout_rate = custom_dropout_rate\n",
    "        self.num_input_words = num_input_words\n",
    "        self.num_target_words = num_target_words\n",
    "        self.input_embedding_matrix = input_embedding_matrix\n",
    "        self.target_embedding_matrix = target_embedding_matrix\n",
    "        self.embedding_size = 300\n",
    "        self.input_pad_len = 80\n",
    "        self.target_pad_len = 60\n",
    "        \n",
    "        encoder_inputs = Input(shape=(self.input_pad_len,))\n",
    "        x = CustomDropout(1.0, custom_dropout_rate)(encoder_inputs)\n",
    "        # x = PositionalEmbedding(input_pad_len, num_input_words + 1, embedding_size)(encoder_inputs)\n",
    "        encoder_embedding = Embedding(input_dim = num_input_words + 1, output_dim = embedding_size, mask_zero = True, weights = [input_embedding_matrix], trainable = False)(x)\n",
    "        encoder_pos_encoding = SinePositionEncoding()(encoder_embedding)\n",
    "        x = encoder_embedding + encoder_pos_encoding\n",
    "        # encoder_outputs = TransformerEncoder(embedding_size, latent_dim, num_heads)(x)\n",
    "        for i in range(num_transformer_layers):\n",
    "            xt = MultiHeadAttention(num_heads=num_heads, key_dim=self.embedding_size)(x, x, x)\n",
    "            x = LayerNormalization()(x + xt)\n",
    "            xt = Dense(self.latent_dim, activation=\"relu\") (x)\n",
    "            xt = Dense(self.embedding_size) (xt)\n",
    "            x = LayerNormalization() (x + xt)\n",
    "        encoder_outputs = x\n",
    "        self.encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "        \n",
    "        decoder_inputs = Input(shape=(self.target_pad_len,))\n",
    "        encoded_seq_inputs = Input(shape=(self.input_pad_len, self.embedding_size))\n",
    "        # x = PositionalEmbedding(target_pad_len, num_target_words + 1, embedding_size)(decoder_inputs)\n",
    "        decoder_embedding = Embedding(input_dim = num_input_words + 1, output_dim = self.embedding_size, mask_zero = True, trainable = True)(decoder_inputs)\n",
    "        decoder_pos_encoding = SinePositionEncoding()(decoder_embedding)\n",
    "        x = decoder_embedding + decoder_pos_encoding\n",
    "        # x = TransformerDecoder(embedding_size, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "        for i in range(num_transformer_layers):\n",
    "            causal_mask = self.get_causal_attention_mask(x)\n",
    "            xt = MultiHeadAttention(num_heads=num_heads, key_dim=self.embedding_size) (x, x, x, attention_mask=causal_mask)\n",
    "            x = LayerNormalization() (x + xt)\n",
    "            xt = MultiHeadAttention(num_heads=num_heads, key_dim=self.embedding_size) (x, encoded_seq_inputs, encoded_seq_inputs)\n",
    "            x = LayerNormalization() (x + xt)\n",
    "            xt = Dense(latent_dim, activation=\"relu\") (x)\n",
    "            xt = Dense(self.embedding_size) (xt)\n",
    "            x = LayerNormalization() (x + xt)\n",
    "        x = layers.Dropout(self.dropout_rate)(x)\n",
    "        decoder_outputs = layers.Dense(num_target_words + 1, activation=\"softmax\")(x)\n",
    "        self.decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "        \n",
    "        decoder_outputs = self.decoder([decoder_inputs, encoder_outputs])\n",
    "        self.transformer = keras.Model(\n",
    "            [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    "        )\n",
    "        print(self.transformer.summary(expand_nested=True))\n",
    "        utils.plot_model(self.transformer, show_shapes=True, expand_nested=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.transformer([x[0], x[1]])\n",
    "        \n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "    def translate(self, encoder_input, decoder_input):\n",
    "        return self.transformer([encoder_input, decoder_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06158376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trenira model na train_data i evaluira ga na val_data\n",
    "#Embedding learning rate je poseban learning_rate koji se koristi u embedding slojevima, iz razloga sto oni vec imaju pretrenirane podatke za pocetne vrednosti\n",
    "#Model se trenira dok val_loss ne krene da raste, i cuva tezine epohe koja ima najbolji val_loss\n",
    "#Koristi se u cv_evaluate, napravio funkciju jer inace dolazi do prekoracenje GPU RAMa, neko je napisao da je do unakrsne validacije\n",
    "def train_and_evaluate(train_data, val_data, epochs = 200, batch_size = 128, learning_rate = 0.001, latent_dim = 256, dropout_rate = 0.5, embedding_learning_rate = 0.001):\n",
    "     \n",
    "     input_texts, target_texts = clean_texts(train_data.iloc[:,1], train_data.iloc[:,0])\n",
    "     input_word_index, target_word_index, max_input_seq_len, max_target_seq_len = analyse_texts(input_texts, target_texts)\n",
    "     input_pad_len = 80\n",
    "     target_pad_len = 60\n",
    "     num_input_words = len(input_word_index) - 1\n",
    "     num_target_words = len(target_word_index) - 1\n",
    "     #print(num_input_words)\n",
    "     inverted_input_word_index = {value: key for key,value in input_word_index.items()}\n",
    "     inverted_target_word_index = {value: key for (key,value) in target_word_index.items()}\n",
    "     #print(len(inverted_input_word_index))\n",
    "     input_embedding_matrix, target_embedding_matrix = load_embedding_data_get_matrices(inverted_input_word_index, inverted_target_word_index)\n",
    "     print('Embeddings loaded.')\n",
    "     encoder_input_data, decoder_input_data, decoder_output_data = create_model_data(input_texts, target_texts, input_word_index, target_word_index, input_pad_len, target_pad_len)\n",
    "     #print(input_embedding_matrix.shape)\n",
    "     \n",
    "     input_texts_val, target_texts_val = clean_texts(val_data.iloc[:,1], val_data.iloc[:,0])\n",
    "     encoder_input_data_val, decoder_input_data_val, decoder_output_data_val = create_model_data(input_texts_val, target_texts_val, input_word_index, target_word_index, input_pad_len, target_pad_len)\n",
    "     \n",
    "     #print('Data preprocessed.')\n",
    "     model_transformer = Transformer_Translation_Model(num_input_words, num_target_words, input_embedding_matrix, target_embedding_matrix, latent_dim = latent_dim, dropout_rate = dropout_rate)\n",
    "     #print('Model loaded.')\n",
    "     other_layers = model_transformer.layers #Mora da se prilagodi za transformer\n",
    "     embedding_layers = [] #Paznja! Mora se prilagoditi svaki put kad se model menja\n",
    "\n",
    "     optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers = [(Adam(learning_rate), other_layers), (Adam(embedding_learning_rate), embedding_layers)])\n",
    "     model_transformer.compile(optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "     #print('Model compiled.')\n",
    "     history = model_transformer.fit([encoder_input_data, decoder_input_data], decoder_output_data, validation_data = ([encoder_input_data_val, decoder_input_data_val], decoder_output_data_val), epochs = epochs, batch_size = batch_size, callbacks = [early_stopping], verbose = 1)\n",
    "     #print('Model fit.')\n",
    "     best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "     \n",
    "     #print('Best epoch: ', best_epoch)\n",
    "     best_loss = np.min(history.history['val_loss'])\n",
    "     #print('Best loss:', best_loss)\n",
    "     #print(model_transformer.evaluate([encoder_input_data_val, decoder_input_data_val], decoder_output_data_val))\n",
    "     K.clear_session()\n",
    "     wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1 = evaluate(model_transformer, input_texts_val, target_texts_val, input_word_index, target_word_index, inverted_target_word_index, input_pad_len, target_pad_len)\n",
    "     K.clear_session()\n",
    "     return best_epoch, best_loss, wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4d380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trenira po model za svaki fold, racuna WER, smooth BLEU(1,2,3,4), kao i val_loss i broj epoha do konvergencije\n",
    "#Vraca podatke iz svake instance modela, odnosno za svaki fold, da bi se dalje procesirale\n",
    "def cv_evaluate(train_val_data = None, df_folds = None, folds = 5, epochs = 200, batch_size = 32, learning_rate = 0.001, latent_dim = 256, dropout_rate = 0.5, embedding_learning_rate = None):\n",
    "    if embedding_learning_rate == None:\n",
    "        embedding_learning_rate = learning_rate\n",
    "    if df_folds == None:\n",
    "        df_np = train_val_data.to_numpy()\n",
    "        np.random.shuffle(df_np)\n",
    "        total_size = df_np.shape[0]\n",
    "        fold_size = total_size/folds\n",
    "        df_folds = [df_np[int(i*fold_size):int((i+1)*fold_size),] for i in range(folds)]\n",
    "    #input_word_embeddings, target_word_embeddings = load_embedding_data() #Doslo je do prekoracenje memorije\n",
    "    losses = []\n",
    "    best_epochs = []\n",
    "    wers = []\n",
    "    smooth_bleu1s = []\n",
    "    smooth_bleu2s = []\n",
    "    smooth_bleu3s = []\n",
    "    smooth_bleu4s = []\n",
    "    for i in range(folds):\n",
    "        train_folds = [fold for j, fold in enumerate(df_folds) if j!=i]\n",
    "        train_folds_pd = [pd.DataFrame(data = fold) for fold in train_folds]\n",
    "        train_data = pd.concat(train_folds_pd)\n",
    "        val_data = pd.DataFrame(df_folds[i])\n",
    "        print('Current Latent Dim:', latent_dim)\n",
    "        print('Current Dropout Rate: ', dropout_rate)\n",
    "        print('Current Fold: {}/{}'.format(i+1, folds))\n",
    "        print('Current Learning Rate: ', learning_rate)\n",
    "        print('Current Learning Rate Multiplier: ', embedding_learning_rate/learning_rate)\n",
    "        \n",
    "        best_epoch, best_loss, wer, smooth_bleu4, smooth_bleu3, smooth_bleu2, smooth_bleu1 = train_and_evaluate(train_data, val_data, epochs = epochs, batch_size = batch_size, learning_rate = learning_rate, latent_dim = latent_dim, dropout_rate = dropout_rate, embedding_learning_rate = embedding_learning_rate)\n",
    "        best_epochs.append(best_epoch)\n",
    "        losses.append(best_loss)\n",
    "        wers.append(wer)\n",
    "        smooth_bleu4s.append(smooth_bleu4)\n",
    "        smooth_bleu3s.append(smooth_bleu3)\n",
    "        smooth_bleu2s.append(smooth_bleu2)\n",
    "        smooth_bleu1s.append(smooth_bleu1)\n",
    "    return best_epochs, losses, wers, smooth_bleu4s, smooth_bleu3s, smooth_bleu2s, smooth_bleu1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8f5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluira modele za razlicite vrednosti latent_dim i dropout_rate\n",
    "#U 3d matrici cuva rezultate, treca dimenzija predstavlja vrednosti za razlicite foldove, uprosecavanjem se dobija zeljena metrika\n",
    "#Isto cuva i broj epoha do konvergencije \n",
    "def cv_grid_search(df, dropout_rates, latent_dims, epochs = 200, learning_rate = 0.0002, folds = 5):\n",
    "    df_np = df.to_numpy()\n",
    "    np.random.shuffle(df_np)\n",
    "    total_size = df_np.shape[0]\n",
    "    fold_size = total_size/folds\n",
    "    df_folds = [df_np[int(i*fold_size):int((i+1)*fold_size),] for i in range(folds)]\n",
    "    \n",
    "    loss_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    epoch_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    wer_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu4_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu3_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu2_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    smooth_bleu1_matrix = np.zeros((len(latent_dims),len(dropout_rates), folds))\n",
    "    for i in range(len(latent_dims)):\n",
    "        for j in range(len(dropout_rates)):\n",
    "            best_epochs, losses, wers, smooth_bleu4s, smooth_bleu3s, smooth_bleu2s, smooth_bleu1s = cv_evaluate(df_folds = df_folds, folds = folds, epochs = epochs, learning_rate = learning_rate, latent_dim = latent_dims[i], dropout_rate = dropout_rates[j])\n",
    "            print(losses)\n",
    "            print(best_epochs)\n",
    "            loss_matrix[i,j,:] = losses\n",
    "            epoch_matrix[i,j,:] = best_epochs\n",
    "            wer_matrix[i,j,:] = wers\n",
    "            smooth_bleu4_matrix[i,j,:] = smooth_bleu4s\n",
    "            smooth_bleu3_matrix[i,j,:] = smooth_bleu3s\n",
    "            smooth_bleu2_matrix[i,j,:] = smooth_bleu2s\n",
    "            smooth_bleu1_matrix[i,j,:] = smooth_bleu1s\n",
    "    #Pakuju se rezultati u dictionary radi intuitivnijeg poziva funkcije\n",
    "    metrics_dict = {'loss': loss_matrix, 'epoch': epoch_matrix, 'wer': wer_matrix, 'smooth_bleu4': smooth_bleu4_matrix, 'smooth_bleu3': smooth_bleu3_matrix, 'smooth_bleu2': smooth_bleu2_matrix, 'smooth_bleu1': smooth_bleu1_matrix }\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1d9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/PHOENIX-2014-T.train.corpus.csv', sep='|')\n",
    "df_train = df_train.drop(columns=['name','video','start','end','speaker'])\n",
    "train_size = df_train.shape[0]\n",
    "#Orth je glossovana recenica, translation je originalna engleska\n",
    "\n",
    "df_val = pd.read_csv('data/PHOENIX-2014-T.dev.corpus.csv', sep = '|')\n",
    "df_val.drop(columns = ['name', 'video', 'start', 'end', 'speaker'], inplace = True)\n",
    "val_size = df_val.shape[0]\n",
    "\n",
    "df_test = pd.read_csv('data/PHOENIX-2014-T.test.corpus.csv', sep = '|')\n",
    "df_test.drop(columns = ['name', 'video', 'start', 'end', 'speaker'], inplace = True)\n",
    "test_size = df_test.shape[0]\n",
    "\n",
    "df_train_val = pd.concat([df_train, df_val])\n",
    "df_full = pd.concat([df_train_val, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c341b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametri za optimizaciju: dropout rate i latentna dimenzija\n",
    "dropout_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "latent_dims = [256, 512, 1024] #Treba probati i vecu latentnu dimenziju i dropout rate, posto optimalna vrednost ispada najveca\n",
    "learning_rate = 0.0002\n",
    "folds = 5\n",
    "\n",
    "num_heads = 8\n",
    "num_transformer_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036af7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Latent Dim: 256\n",
      "Current Dropout Rate:  0.5\n",
      "Current Fold: 1/5\n",
      "Current Learning Rate:  0.0002\n",
      "Current Learning Rate Multiplier:  1.0\n",
      "Embeddings loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 22:06:43.764925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.779721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.779851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.780528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.780627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.780704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.823451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.823577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.823654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:06:43.823715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9545 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 80)]                 0         []                            \n",
      "                                                                                                  \n",
      " custom_dropout (CustomDrop  (None, 80)                   0         ['input_1[0][0]']             \n",
      " out)                                                                                             \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 80, 300)              595500    ['custom_dropout[0][0]']      \n",
      "                                                                                                  \n",
      " sine_position_encoding (Si  (None, 80, 300)              0         ['embedding[0][0]']           \n",
      " nePositionEncoding)                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 80, 300)              0         ['embedding[0][0]',           \n",
      " Lambda)                                                             'sine_position_encoding[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 80, 300)              2887500   ['tf.__operators__.add[0][0]',\n",
      " iHeadAttention)                                                     'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 80, 300)              0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 80, 300)              600       ['tf.__operators__.add_1[0][0]\n",
      " Normalization)                                                     ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 80, 256)              77056     ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 80, 300)              77100     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 80, 300)              0         ['layer_normalization[0][0]', \n",
      " OpLambda)                                                           'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 60)]                 0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 80, 300)              600       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, 60, 732)              6746788   ['input_2[0][0]',             \n",
      "                                                                     'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_2 (InputLayer)       [(None, 60)]                 0         []                           |\n",
      "|                                                                                                |\n",
      "| embedding_1 (Embedding)    (None, 60, 300)              595500    []                           |\n",
      "|                                                                                                |\n",
      "| sine_position_encoding_1   (None, 60, 300)              0         []                           |\n",
      "| (SinePositionEncoding)                                                                         |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.add_3 (T  (None, 60, 300)              0         []                           |\n",
      "| FOpLambda)                                                                                     |\n",
      "|                                                                                                |\n",
      "| tf.compat.v1.shape (TFOpL  (3,)                         0         []                           |\n",
      "| ambda)                                                                                         |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.getitem_  ()                           0         []                           |\n",
      "| 1 (SlicingOpLambda)                                                                            |\n",
      "|                                                                                                |\n",
      "| tf.range (TFOpLambda)      (60,)                        0         []                           |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.getitem_  (60, 1)                      0         []                           |\n",
      "| 2 (SlicingOpLambda)                                                                            |\n",
      "|                                                                                                |\n",
      "| tf.range_1 (TFOpLambda)    (60,)                        0         []                           |\n",
      "|                                                                                                |\n",
      "| tf.math.greater_equal (TF  (60, 60)                     0         []                           |\n",
      "| OpLambda)                                                                                      |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.getitem   ()                           0         []                           |\n",
      "| (SlicingOpLambda)                                                                              |\n",
      "|                                                                                                |\n",
      "| tf.cast (TFOpLambda)       (60, 60)                     0         []                           |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.getitem_  ()                           0         []                           |\n",
      "| 3 (SlicingOpLambda)                                                                            |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.getitem_  ()                           0         []                           |\n",
      "| 4 (SlicingOpLambda)                                                                            |\n",
      "|                                                                                                |\n",
      "| tf.expand_dims (TFOpLambd  (1,)                         0         []                           |\n",
      "| a)                                                                                             |\n",
      "|                                                                                                |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| tf.reshape (TFOpLambda)    (1, 60, 60)                  0         []                           |\n",
      "|                                                                                                |\n",
      "| tf.concat (TFOpLambda)     (3,)                         0         []                           |\n",
      "|                                                                                                |\n",
      "| tf.tile (TFOpLambda)       (None, 60, 60)               0         []                           |\n",
      "|                                                                                                |\n",
      "| multi_head_attention_1 (M  (None, 60, 300)              2887500   []                           |\n",
      "| ultiHeadAttention)                                                                             |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.add_4 (T  (None, 60, 300)              0         []                           |\n",
      "| FOpLambda)                                                                                     |\n",
      "|                                                                                                |\n",
      "| layer_normalization_2 (La  (None, 60, 300)              600       []                           |\n",
      "| yerNormalization)                                                                              |\n",
      "|                                                                                                |\n",
      "| input_3 (InputLayer)       [(None, 80, 300)]            0         []                           |\n",
      "|                                                                                                |\n",
      "| multi_head_attention_2 (M  (None, 60, 300)              2887500   []                           |\n",
      "| ultiHeadAttention)                                                                             |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.add_5 (T  (None, 60, 300)              0         []                           |\n",
      "| FOpLambda)                                                                                     |\n",
      "|                                                                                                |\n",
      "| layer_normalization_3 (La  (None, 60, 300)              600       []                           |\n",
      "| yerNormalization)                                                                              |\n",
      "|                                                                                                |\n",
      "| dense_2 (Dense)            (None, 60, 256)              77056     []                           |\n",
      "|                                                                                                |\n",
      "| dense_3 (Dense)            (None, 60, 300)              77100     []                           |\n",
      "|                                                                                                |\n",
      "| tf.__operators__.add_6 (T  (None, 60, 300)              0         []                           |\n",
      "| FOpLambda)                                                                                     |\n",
      "|                                                                                                |\n",
      "| layer_normalization_4 (La  (None, 60, 300)              600       []                           |\n",
      "| yerNormalization)                                                                              |\n",
      "|                                                                                                |\n",
      "| dropout (Dropout)          (None, 60, 300)              0         []                           |\n",
      "|                                                                                                |\n",
      "| dense_4 (Dense)            (None, 60, 732)              220332    []                           |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "==================================================================================================\n",
      "Total params: 10385144 (39.62 MB)\n",
      "Trainable params: 9789644 (37.34 MB)\n",
      "Non-trainable params: 595500 (2.27 MB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 22:06:47.212437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-15 22:06:47.258853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-09-15 22:06:47.280510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f11f01b43e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-15 22:06:47.280527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2023-09-15 22:06:47.283318: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-15 22:06:47.369151: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 13s 44ms/step - loss: 0.9342 - acc: 0.8511 - val_loss: 0.7435 - val_acc: 0.8641\n",
      "Epoch 2/200\n",
      "191/191 [==============================] - 4s 20ms/step - loss: 0.7342 - acc: 0.8670 - val_loss: 0.6845 - val_acc: 0.8701\n",
      "Epoch 3/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.6824 - acc: 0.8713 - val_loss: 0.6465 - val_acc: 0.8749\n",
      "Epoch 4/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.6250 - acc: 0.8794 - val_loss: 0.5831 - val_acc: 0.8846\n",
      "Epoch 5/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.5745 - acc: 0.8864 - val_loss: 0.5386 - val_acc: 0.8922\n",
      "Epoch 6/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.5409 - acc: 0.8914 - val_loss: 0.5178 - val_acc: 0.8944\n",
      "Epoch 7/200\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.5168 - acc: 0.8951 - val_loss: 0.5050 - val_acc: 0.8966\n",
      "Epoch 8/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.4988 - acc: 0.8975 - val_loss: 0.4932 - val_acc: 0.8984\n",
      "Epoch 9/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4817 - acc: 0.8997 - val_loss: 0.4754 - val_acc: 0.9003\n",
      "Epoch 10/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.4675 - acc: 0.9014 - val_loss: 0.4668 - val_acc: 0.9025\n",
      "Epoch 11/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4546 - acc: 0.9035 - val_loss: 0.4588 - val_acc: 0.9033\n",
      "Epoch 12/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4427 - acc: 0.9053 - val_loss: 0.4577 - val_acc: 0.9034\n",
      "Epoch 13/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4320 - acc: 0.9065 - val_loss: 0.4461 - val_acc: 0.9060\n",
      "Epoch 14/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4206 - acc: 0.9082 - val_loss: 0.4462 - val_acc: 0.9053\n",
      "Epoch 15/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4114 - acc: 0.9095 - val_loss: 0.4456 - val_acc: 0.9059\n",
      "Epoch 16/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.4030 - acc: 0.9108 - val_loss: 0.4355 - val_acc: 0.9079\n",
      "Epoch 17/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3927 - acc: 0.9127 - val_loss: 0.4342 - val_acc: 0.9069\n",
      "Epoch 18/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3867 - acc: 0.9134 - val_loss: 0.4307 - val_acc: 0.9084\n",
      "Epoch 19/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3794 - acc: 0.9143 - val_loss: 0.4287 - val_acc: 0.9087\n",
      "Epoch 20/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3717 - acc: 0.9157 - val_loss: 0.4254 - val_acc: 0.9089\n",
      "Epoch 21/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3627 - acc: 0.9172 - val_loss: 0.4278 - val_acc: 0.9091\n",
      "Epoch 22/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3577 - acc: 0.9178 - val_loss: 0.4352 - val_acc: 0.9088\n",
      "Epoch 23/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3515 - acc: 0.9189 - val_loss: 0.4240 - val_acc: 0.9097\n",
      "Epoch 24/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3433 - acc: 0.9203 - val_loss: 0.4251 - val_acc: 0.9103\n",
      "Epoch 25/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3377 - acc: 0.9215 - val_loss: 0.4190 - val_acc: 0.9105\n",
      "Epoch 26/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3309 - acc: 0.9223 - val_loss: 0.4181 - val_acc: 0.9118\n",
      "Epoch 27/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3263 - acc: 0.9230 - val_loss: 0.4284 - val_acc: 0.9106\n",
      "Epoch 28/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3197 - acc: 0.9242 - val_loss: 0.4234 - val_acc: 0.9119\n",
      "Epoch 29/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.3166 - acc: 0.9248 - val_loss: 0.4261 - val_acc: 0.9110\n",
      "Epoch 30/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3099 - acc: 0.9259 - val_loss: 0.4252 - val_acc: 0.9120\n",
      "Epoch 31/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.3021 - acc: 0.9271 - val_loss: 0.4277 - val_acc: 0.9116\n",
      "Epoch 32/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.2980 - acc: 0.9276 - val_loss: 0.4368 - val_acc: 0.9120\n",
      "Epoch 33/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.2935 - acc: 0.9285 - val_loss: 0.4319 - val_acc: 0.9120\n",
      "Epoch 34/200\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.2870 - acc: 0.9298 - val_loss: 0.4361 - val_acc: 0.9103\n",
      "Epoch 35/200\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.2837 - acc: 0.9302 - val_loss: 0.4386 - val_acc: 0.9117\n",
      "Epoch 36/200\n",
      "189/191 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9317Restoring model weights from the end of the best epoch: 26.\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.2782 - acc: 0.9317 - val_loss: 0.4354 - val_acc: 0.9125\n",
      "Epoch 36: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensor is unhashable. Instead, use tensor.ref() as the key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_116068/2752926902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Metrika nad kojom se vrsi selekcija je smooth BLEU4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0maverage_bleu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_bleu4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_116068/4102772494.py\u001b[0m in \u001b[0;36mcv_grid_search\u001b[0;34m(df, dropout_rates, latent_dims, epochs, learning_rate, folds)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mbest_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu4s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu3s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_116068/4139074194.py\u001b[0m in \u001b[0;36mcv_evaluate\u001b[0;34m(train_val_data, df_folds, folds, epochs, batch_size, learning_rate, latent_dim, dropout_rate, embedding_learning_rate)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current Learning Rate Multiplier: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_learning_rate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mbest_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_116068/552549093.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_data, val_data, epochs, batch_size, learning_rate, latent_dim, dropout_rate, embedding_learning_rate)\u001b[0m\n\u001b[1;32m     41\u001b[0m      \u001b[0;31m#print(model_transformer.evaluate([encoder_input_data_val, decoder_input_data_val], decoder_output_data_val))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m      \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m      \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_texts_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_target_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pad_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_pad_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m      \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bleu1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/faks/mu/txt2gloss/funkcije.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_input_sentences, test_references, input_word_index, target_word_index, inverted_target_word_index, input_pad_len, target_pad_len)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mumlaut_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mtest_references\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mumlaut_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_references\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_target_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pad_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_pad_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0msmooth_bleu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_references\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0msmooth_bleu3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_references\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/faks/mu/txt2gloss/funkcije.py\u001b[0m in \u001b[0;36mtranslate_from_text\u001b[0;34m(model, input_sentences, input_word_index, target_word_index, inverted_target_word_index, input_pad_len, target_pad_len)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdecoder_input_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<Start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0moutput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_target_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0moutput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<End>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/faks/mu/txt2gloss/funkcije.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdecoder_input_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<Start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0moutput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_target_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0moutput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<End>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/faks/mu/txt2gloss/funkcije.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdecoder_input_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<Start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0moutput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_target_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0moutput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<End>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' - '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;31m# EagerTensors are never hashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     raise TypeError(\"Tensor is unhashable. \"\n\u001b[0m\u001b[1;32m   1100\u001b[0m                     \"Instead, use tensor.ref() as the key.\")\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor is unhashable. Instead, use tensor.ref() as the key."
     ]
    }
   ],
   "source": [
    "#Izvrsava se grid search nad hiperparametrima, i radi se unakrsna validacija za evaluaciju performansi\n",
    "#Metrika nad kojom se vrsi selekcija je smooth BLEU4\n",
    "\n",
    "metrics = cv_grid_search(df_train_val, dropout_rates, latent_dims, epochs = 200, learning_rate = learning_rate, folds = folds)\n",
    "average_bleu4 = np.mean(metrics['smooth_bleu4'], axis = -1)\n",
    "\n",
    "plt.title('Average smooth BLEU4, crossvalidated')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.xticks(range(len(dropout_rates)), dropout_rates)\n",
    "plt.ylabel('Latent Dim')\n",
    "plt.yticks(range(len(latent_dims)),latent_dims)\n",
    "plt.imshow(average_bleu4)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
